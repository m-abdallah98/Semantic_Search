{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaNYTfyYaw2u"
      },
      "outputs": [],
      "source": [
        "!pip install transformers tensorflow_text sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bANbAMwkO9AW"
      },
      "source": [
        "## **Downloading Data**\n",
        "### Data from kaggle for a number of srticles and there abstacts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O papers.zip \"https://storage.googleapis.com/kaggle-data-sets/491/9097/compressed/papers.csv.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20230411%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230411T104147Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=2492eb055dbd527ba639bfa4dc87cfe21daf40270c424a2a0893a10806ed7aa19e34406a541d4f5fcdb6870c85034a2e2056ab8eb47a1a6b5a4737fe680a956baafd20f536714d095d81723553b5156227ed2ad05124537d7982d8deb0e6c0cb0256c4170854813ca1ab34d8feaa78e22b2b00a39aa1d4fd9521a68b0fe057f11a6236792d06ee2bf4f9234c97e500aaa3c0403b5e92087cfc0dc2d24217005e5498155be5be637e78f15f75d919c9b0163ca823ba87ba767777eeb1e5da6f1a1702210855f08237ebc41bae297d6dd7516da9acd80e675ca5388decaedf24508d018bc52ae4f8bf93dbff89bf4f16c8b548b209f643771b825565f069e8696d\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4LhGalIbADr",
        "outputId": "04001885-9387-407b-fa1c-617c9e7f7244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-11 23:04:08--  https://storage.googleapis.com/kaggle-data-sets/491/9097/compressed/papers.csv.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20230411%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230411T104147Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=2492eb055dbd527ba639bfa4dc87cfe21daf40270c424a2a0893a10806ed7aa19e34406a541d4f5fcdb6870c85034a2e2056ab8eb47a1a6b5a4737fe680a956baafd20f536714d095d81723553b5156227ed2ad05124537d7982d8deb0e6c0cb0256c4170854813ca1ab34d8feaa78e22b2b00a39aa1d4fd9521a68b0fe057f11a6236792d06ee2bf4f9234c97e500aaa3c0403b5e92087cfc0dc2d24217005e5498155be5be637e78f15f75d919c9b0163ca823ba87ba767777eeb1e5da6f1a1702210855f08237ebc41bae297d6dd7516da9acd80e675ca5388decaedf24508d018bc52ae4f8bf93dbff89bf4f16c8b548b209f643771b825565f069e8696d\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.62.128, 172.253.115.128, 172.253.122.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.62.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 73128650 (70M) [application/zip]\n",
            "Saving to: ‘papers.zip’\n",
            "\n",
            "papers.zip          100%[===================>]  69.74M   162MB/s    in 0.4s    \n",
            "\n",
            "2023-04-11 23:04:09 (162 MB/s) - ‘papers.zip’ saved [73128650/73128650]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/papers.zip\" -d \"/content/data\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMO6cgM_bQWj",
        "outputId": "04299284-bafb-4456-f509-2eab12d02608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/papers.zip\n",
            "  inflating: /content/data/papers.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DcmJ7Ntmahd"
      },
      "source": [
        "## **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "from tqdm.notebook import tqdm\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "import os\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import pprint\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNhlub0cbYez",
        "outputId": "f675d07e-c8b4-4eb0-a385-a3c28c944e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgdg-kYBsjus"
      },
      "source": [
        "## **Reading the data:** \n",
        "### The data is a set of papers saved in a csv file and our main concern would be of the abstract feature\n",
        "### We will give most concern for the the abstract as a proof of concept"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBz2xzwAkV-y"
      },
      "source": [
        "Calling only articles that  have abstracts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/data/papers.csv\"\n",
        "df = pd.read_csv(data_path)\n",
        "atrticles_with_abstract = df[df['abstract'] != 'Abstract Missing']\n",
        "articles = list(atrticles_with_abstract.abstract)"
      ],
      "metadata": {
        "id": "Ly8rr287bm2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYPWkx1YSW20"
      },
      "source": [
        "## Using the pretrained ***Transformer***: \"***all-MiniLM-L6-v2***\" *in preprocessing and producing the output*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzAZOnCHlU8T"
      },
      "source": [
        "We define a set of keywords and use a pre-trained semantic search model to encode them. It then encodes a list of articles using the same semantic search model and computes the cosine similarity between the encoded articles and the encoded keywords. The similarity scores are stored in a list.\n",
        "\n",
        "The \"all-MiniLM-L6-v2\" is based on the MiniLM architecture, which is a small and efficient language model designed to achieve state-of-the-art performance on various natural language processing (NLP) tasks with a small number of parameters.\n",
        "\n",
        "All-MiniLM-L6-v2 is an even smaller version of the MiniLM-L6 model, with a total of 71 million parameters. Despite its smaller size, it has achieved impressive results on various benchmark datasets and tasks in the NLP field."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Encoding keywords and abstracts\n",
        "\n",
        "articles_encoded = model.encode(articles, convert_to_tensor=True)\n",
        "keywords = ['technology', 'business', 'science', 'entertainment', 'politics']\n",
        "\n",
        "\n",
        "# Find the closest 5 abstracts of the articles for each query keyword based on cosine similarity\n",
        "top_k = min(5, len(articles))\n",
        "for query in keywords:\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
        "    cos_scores = util.cos_sim(query_embedding, articles_encoded)[0]\n",
        "    top_results = torch.topk(cos_scores, k=top_k)\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop 5 most similar articles in corpus:\")\n",
        "\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "        print(articles[idx], \"(Score: {:.4f})\".format(score))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGalQBIsbmyy",
        "outputId": "e8fbc158-7275-47a4-c451-3fa4c699f002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: technology\n",
            "\n",
            "Top 5 most similar articles in corpus:\n",
            "Communication between a speaker and hearer will be most efficient when both parties make accurate inferences about the other. We study inference and communication in a television game called Password, where speakers must convey secret words to hearers by providing one-word clues. Our working hypothesis is that human communication is relatively efficient, and we use game show data to examine three predictions. First, we predict that speakers and hearers are both considerate, and that both take the other?s perspective into account. Second, we predict that speakers and hearers are calibrated, and that both make accurate assumptions about the strategy used by the other. Finally, we predict that speakers and hearers are collaborative, and that they tend to share the cognitive burden of communication equally. We find evidence in support of all three predictions, and demonstrate in addition that efficient communication tends to break down when speakers and hearers are placed under time pressure. (Score: 0.2290)\n",
            "We present a characterization of a useful class of skills based on a graphical representation of an agent's interaction with its environment. Our characterization uses betweenness, a measure of centrality on graphs. It may be used directly to form a set of skills suitable for a given environment. More importantly, it serves as a useful guide for developing online, incremental skill discovery algorithms that do not rely on knowing or representing the environment graph in its entirety. (Score: 0.2216)\n",
            "Machine learning offers a fantastically powerful toolkit for building useful complexprediction systems quickly. This paper argues it is dangerous to think ofthese quick wins as coming for free. Using the software engineering frameworkof technical debt, we find it is common to incur massive ongoing maintenancecosts in real-world ML systems. We explore several ML-specific risk factors toaccount for in system design. These include boundary erosion, entanglement,hidden feedback loops, undeclared consumers, data dependencies, configurationissues, changes in the external world, and a variety of system-level anti-patterns. (Score: 0.2142)\n",
            "We introduce skill chaining, a skill discovery method for reinforcement learning agents in continuous domains, that builds chains of skills leading to an end-of-task reward. We demonstrate experimentally that it creates skills that result in performance benefits in a challenging continuous domain. (Score: 0.1856)\n",
            "Machine Learning competitions such as the Netflix Prize have proven reasonably successful as a method of ?crowdsourcing? prediction tasks. But these compe- titions have a number of weaknesses, particularly in the incentive structure they create for the participants. We propose a new approach, called a Crowdsourced Learning Mechanism, in which participants collaboratively ?learn? a hypothesis for a given prediction task. The approach draws heavily from the concept of a prediction market, where traders bet on the likelihood of a future event. In our framework, the mechanism continues to publish the current hypothesis, and par- ticipants can modify this hypothesis by wagering on an update. The critical in- centive property is that a participant will profit an amount that scales according to how much her update improves performance on a released test set. (Score: 0.1819)\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: business\n",
            "\n",
            "Top 5 most similar articles in corpus:\n",
            "Inspired by real-time ad exchanges for online display advertising, we consider the problem of inferring a buyer's value distribution for a good when the buyer is repeatedly interacting with a seller through a posted-price mechanism.  We model the buyer as a strategic agent, whose goal is to maximize her long-term surplus, and we are interested in mechanisms that maximize the seller's long-term revenue. We present seller algorithms that are no-regret when the buyer discounts her future surplus --- i.e. the buyer prefers showing advertisements to users sooner rather than later. We also give a lower bound on regret that increases as the buyer's discounting weakens and shows, in particular, that any seller algorithm will suffer linear regret if there is no discounting. (Score: 0.2570)\n",
            "We visit the following fundamental problem: For a `generic model of consumer choice (namely, distributions over preference lists) and a limited amount of data on how consumers actually make decisions (such as marginal preference information), how may one predict revenues from offering a particular assortment of choices? This problem is central to areas within operations research, marketing and econometrics. We present a framework to answer such questions and design a number of tractable algorithms (from a data and computational standpoint) for the same. (Score: 0.2428)\n",
            "Motivated by real-time advertising exchanges, we analyze the problem of pricing inventory in a repeated posted-price auction. We consider both the cases of a truthful and surplus-maximizing buyer, where the former makes decisions myopically on every round, and the latter may strategically react to our algorithm, forgoing short-term surplus in order to trick the algorithm into setting better prices in the future. We further assume a buyer?s valuation of a good is a function of a context vector that describes the good being sold. We give the first algorithm attaining sublinear (O(T^{2/3})) regret in the contextual setting against a surplus-maximizing buyer. We also extend this result to repeated second-price auctions with multiple buyers. (Score: 0.2399)\n",
            "We present a scalable and robust Bayesian method for demand forecasting in the context of a large e-commerce platform, paying special attention to intermittent and bursty target statistics. Inference is approximated by the Newton-Raphson algorithm, reduced to linear-time Kalman smoothing, which allows us to operate on several orders of magnitude larger problems than previous related work. In a study on large real-world sales datasets, our method outperforms competing approaches on fast and medium moving items. (Score: 0.2392)\n",
            "We study the profit-maximization problem of a monopolistic market-maker who sets two-sided prices in an asset market. The sequential decision problem is hard to solve because the state space is a function. We demonstrate that the belief state is well approximated by a Gaussian distribution. We prove a key monotonicity property of the Gaussian state update which makes the problem tractable, yielding the first optimal sequential market-making algorithm in an established model. The algorithm leads to a surprising insight: an optimal monopolist can provide more liquidity than perfectly competitive market-makers in periods of extreme uncertainty, because a monopolist is willing to absorb initial losses in order to learn a new valuation rapidly so she can extract higher profits later. (Score: 0.2387)\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: science\n",
            "\n",
            "Top 5 most similar articles in corpus:\n",
            "Humans are typically able to infer how many objects their environment contains and to recognize when the same object is encountered twice.  We present a simple statistical model that helps to explain these abilities and evaluate it in three behavioral experiments.  Our first experiment suggests that humans rely on prior knowledge when deciding whether an object token has been previously encountered. Our second and third experiments suggest that humans can infer how many objects they have seen and can learn about categories and their properties even when they are uncertain about which tokens are instances of the same object. (Score: 0.2626)\n",
            "Given one feature of a novel animal, humans readily make inferences about other features of the animal. For example, winged creatures often fly, and creatures that eat fish often live in the water. We explore the knowledge that supports these inferences and compare two approaches. The first approach proposes that humans rely on abstract representations of dependency relationships between features, and is formalized here as a graphical model.  The second approach proposes that humans rely on specific knowledge of previously encountered animals, and is formalized here as a family of exemplar models. We evaluate these models using a task where participants reason about chimeras, or animals with pairs of features that have not previously been observed to co-occur. The results support the hypothesis that humans rely on explicit representations of relationships between features. (Score: 0.2381)\n",
            "What can we infer from hearing an object falling onto the ground? Based on knowledge of the physical world, humans are able to infer rich information from such limited data: rough shape of the object, its material, the height of falling, etc. In this paper, we aim to approximate such competency. We first mimic the human knowledge about the physical world using a fast physics-based generative model. Then, we present an analysis-by-synthesis approach to infer properties of the falling object. We further approximate human past experience by directly mapping audio to object properties using deep learning with self-supervision. We evaluate our method through behavioral studies, where we compare human predictions with ours on inferring object shape, material, and initial height of falling. Results show that our method achieves near-human performance, without any annotations. (Score: 0.2376)\n",
            "Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains. (Score: 0.2356)\n",
            "People often learn from others' demonstrations, and classic inverse reinforcement learning (IRL) algorithms have brought us closer to realizing this capacity in machines. In contrast, teaching by demonstration has been less well studied computationally. Here, we develop a novel Bayesian model for teaching by demonstration. Stark differences arise when demonstrators are intentionally teaching a task versus simply performing a task. In two experiments, we show that human participants systematically modify their teaching behavior consistent with the predictions of our model. Further, we show that even standard IRL algorithms benefit when learning from behaviors that are intentionally pedagogical. We conclude by discussing IRL algorithms that can take advantage of intentional pedagogy. (Score: 0.2311)\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: entertainment\n",
            "\n",
            "Top 5 most similar articles in corpus:\n",
            "Communication between a speaker and hearer will be most efficient when both parties make accurate inferences about the other. We study inference and communication in a television game called Password, where speakers must convey secret words to hearers by providing one-word clues. Our working hypothesis is that human communication is relatively efficient, and we use game show data to examine three predictions. First, we predict that speakers and hearers are both considerate, and that both take the other?s perspective into account. Second, we predict that speakers and hearers are calibrated, and that both make accurate assumptions about the strategy used by the other. Finally, we predict that speakers and hearers are collaborative, and that they tend to share the cognitive burden of communication equally. We find evidence in support of all three predictions, and demonstrate in addition that efficient communication tends to break down when speakers and hearers are placed under time pressure. (Score: 0.3333)\n",
            "In many machine learning applications, submodular functions have been used as a model for evaluating the utility or payoff of a set such as news items to recommend, sensors to deploy in a terrain, nodes to influence in a social network, to name a few. At the heart of all these applications is the assumption that the underlying utility/payoff function is known a priori, hence maximizing it is in principle possible. In real life situations, however, the utility function is not fully known in advance and can only be estimated via interactions. For instance, whether a user likes a movie or not can be reliably evaluated only after it was shown to her. Or, the range of influence of a user in a social network can be estimated only after she is selected to advertise the product. We model such problems as an interactive submodular bandit optimization, where in each round we receive a context (e.g., previously selected movies) and have to choose an action (e.g., propose a new movie). We then receive a noisy feedback about the utility of the action (e.g., ratings) which we model as a submodular function over the context-action space. We develop SM-UCB that efficiently trades off exploration (collecting more data) and exploration (proposing a good action given gathered data) and achieves a $O(\\sqrt{T})$ regret bound after $T$ rounds of interaction. Given a bounded-RKHS norm kernel over the context-action-payoff space that governs the smoothness of the utility function, SM-UCB keeps an upper-confidence bound on the payoff function that allows it to asymptotically achieve no-regret. Finally, we evaluate our results on four concrete applications, including movie recommendation (on the MovieLense data set), news recommendation (on Yahoo! Webscope dataset), interactive influence maximization (on a subset of the Facebook network), and personalized data summarization (on Reuters Corpus). In all these applications, we observe that SM-UCB consistently outperforms the prior art. (Score: 0.2588)\n",
            "In imperfect-information games, the optimal strategy in a subgame may depend on the strategy in other, unreached subgames. Thus a subgame cannot be solved in isolation and must instead consider the strategy for the entire game as a whole, unlike perfect-information games. Nevertheless, it is possible to first approximate a solution for the whole game and then improve it by solving individual subgames. This is referred to as subgame solving. We introduce subgame-solving techniques that outperform prior methods both in theory and practice. We also show how to adapt them, and past subgame-solving techniques, to respond to opponent actions that are outside the original action abstraction; this significantly outperforms the prior state-of-the-art approach, action translation. Finally, we show that subgame solving can be repeated as the game progresses down the game tree, leading to far lower exploitability. These techniques were a key component of Libratus, the first AI to defeat top humans in heads-up no-limit Texas hold'em poker. (Score: 0.2506)\n",
            "Humanity faces numerous problems of common-pool resource appropriation. This class of multi-agent social dilemma includes the problems of ensuring sustainable use of fresh water, common fisheries, grazing pastures, and irrigation systems. Abstract models of common-pool resource appropriation based on non-cooperative game theory predict that self-interested agents will generally fail to find socially positive equilibria---a phenomenon called the tragedy of the commons. However, in reality, human societies are sometimes able to discover and implement stable cooperative solutions. Decades of behavioral game theory research have sought to uncover aspects of human behavior that make this possible. Most of that work was based on laboratory experiments where participants only make a single choice: how much to appropriate. Recognizing the importance of spatial and temporal resource dynamics, a recent trend has been toward experiments in more complex real-time video game-like environments. However, standard methods of non-cooperative game theory can no longer be used to generate predictions for this case. Here we show that deep reinforcement learning can be used instead. To that end, we study the emergent behavior of groups of independently learning agents in a partially observed Markov game modeling common-pool resource  appropriation. Our experiments highlight the importance of trial-and-error learning in common-pool resource appropriation and shed light on the relationship between exclusion, sustainability, and inequality. (Score: 0.2421)\n",
            "Point processes are powerful tools to model user activities and have a plethora of applications in social sciences. Predicting user activities based on point processes is a central problem. However, existing works are mostly problem specific, use heuristics, or simplify the stochastic nature of point processes. In this paper, we propose a framework that provides an unbiased estimator of the probability mass function of point processes. In particular, we design a key reformulation of the prediction problem, and further derive a differential-difference equation to compute a conditional probability mass function. Our framework is applicable to general point processes and prediction tasks, and achieves superb predictive and efficiency performance in diverse real-world applications compared to state-of-arts. (Score: 0.2324)\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: politics\n",
            "\n",
            "Top 5 most similar articles in corpus:\n",
            "We consider problems for which one has incomplete binary matrices that evolve with time (e.g., the votes of legislators on particular legislation, with each year characterized by a different such matrix). An objective of such analysis is to infer structure and inter-relationships underlying the matrices, here defined by latent features associated with each axis of the matrix. In addition, it is assumed that documents are available for the entities associated with at least one of the matrix axes. By jointly analyzing the matrices and documents, one may be used to inform the other within the analysis, and the model offers the opportunity to predict matrix values (e.g., votes) based only on an associated document (e.g., legislation). The research presented here merges two areas of machine-learning that have previously been investigated separately: incomplete-matrix analysis and topic modeling. The analysis is performed from a Bayesian perspective, with efficient inference constituted via Gibbs sampling. The framework is demonstrated by considering all voting data and available documents (legislation) during the 220-year lifetime of the United States Senate and House of Representatives. (Score: 0.2452)\n",
            "We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers' positions on specific political issues.  Our model can be used to explore how a lawmaker's voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model's utility in interpreting an inherently multi-dimensional space. (Score: 0.2369)\n",
            "We consider a  two-player sequential game in which agents have the same reward function but may disagree on the transition probabilities of an underlying Markovian model of the world. By committing to play a specific policy, the agent with the correct model can steer the behavior of the other agent, and seek to improve utility. We model this setting as a multi-view decision process, which we use to formally analyze the positive effect of steering policies. Furthermore, we develop an algorithm for computing the agents' achievable joint policy, and we experimentally show that it can lead to a large utility increase when the agents' models diverge. (Score: 0.2327)\n",
            "Inspired by a two-level theory that unifies agenda setting and ideological  framing, we propose supervised hierarchical latent Dirichlet allocation  (SHLDA) which jointly captures documents' multi-level topic structure and  their polar response variables. Our model extends the nested Chinese restaurant  process to discover a tree-structured topic hierarchy and uses both per-topic  hierarchical and per-word lexical regression parameters to model the response  variables. Experiments in a political domain and on sentiment analysis tasks  show that SHLDA improves predictive accuracy while adding a new dimension of  insight into how topics under discussion are framed. (Score: 0.2165)\n",
            "We investigate the power of voting among diverse, randomized software agents. With teams of computer Go agents in mind, we develop a novel theoretical model of two-stage noisy voting that builds on recent work in machine learning. This model allows us to reason about a collection of agents with different biases (determined by the first-stage noise models), which, furthermore, apply randomized algorithms to evaluate alternatives and produce votes (captured by the second-stage noise models). We analytically demonstrate that a uniform team, consisting of multiple instances of any single agent, must make a significant number of mistakes, whereas a diverse team converges to perfection as the number of agents grows. Our experiments, which pit teams of computer Go agents against strong agents, provide evidence for the effectiveness of voting when agents are diverse. (Score: 0.2136)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzKe-V5mVWPT"
      },
      "source": [
        "# **Extracting Hot-keywords from articles**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQqtW8w6VphI"
      },
      "source": [
        "### Classical processing to prepare the data for extracting the hot keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veW8CaS1HaH-"
      },
      "outputs": [],
      "source": [
        "#Creating a list of custom stopwords that are most common and repeated in almost all papers\n",
        "new_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\", \n",
        "             \"show\", \"result\", \"large\", \n",
        "             \"also\", \"one\", \"two\", \"three\", \n",
        "             \"four\", \"five\", \"seven\",\"eight\",\"nine\"]\n",
        "stop_words = list(stop_words.union(new_words))\n",
        "\n",
        "def pre_process(text):\n",
        "    \"\"\"\n",
        "    Preprocesses the input text by converting to lowercase, removing tags, special characters and digits, \n",
        "    removing stop words and words less than three letters,\n",
        "    and lemmatizing the tokens.\n",
        "    \n",
        "    Args: text: str, the input text to be preprocessed.\n",
        "    \n",
        "    Returns: tokens: list of str, the preprocessed tokens.\n",
        "    \"\"\"\n",
        "    \n",
        "    # lowercase\n",
        "    text=text.lower()\n",
        "    \n",
        "    #remove tags\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "    \n",
        "    # remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    \n",
        "    ##Convert to list from string\n",
        "    text = text.split()\n",
        "    \n",
        "    # remove stopwords\n",
        "    text = [word for word in text if word not in stop_words]\n",
        "\n",
        "    # remove words less than three letters\n",
        "    text = [word for word in text if len(word) >= 3]\n",
        "\n",
        "    # lemmatize\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    tokens = [lmtzr.lemmatize(word) for word in text]\n",
        "    \n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqZ-Zfl4zylw"
      },
      "outputs": [],
      "source": [
        "# Get the top N articles\n",
        "n_articles = 5\n",
        "top_articles = atrticles_with_abstract.iloc[top_results.indices.tolist()[:n_articles]]\n",
        "# Join the article texts into a single string\n",
        "article_text = ' '.join(list(top_articles['abstract']))\n",
        "\n",
        "tokens = pre_process(article_text)\n",
        "\n",
        "# Create a frequency distribution of the tokens\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "\n",
        "# Get the top 10 most frequent tokens\n",
        "top_tokens = fdist.most_common(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX_JFhMiWgte"
      },
      "source": [
        "## Here are the 10 most common keywords in the articles "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IFHphAqbmxe",
        "outputId": "5a057245-9f6f-4d39-b84d-ef63ba55ae9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('model', 14),\n",
              " ('agent', 13),\n",
              " ('matrix', 8),\n",
              " ('analysis', 5),\n",
              " ('document', 5),\n",
              " ('topic', 5),\n",
              " ('voting', 5),\n",
              " ('team', 4),\n",
              " ('vote', 3),\n",
              " ('legislation', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}